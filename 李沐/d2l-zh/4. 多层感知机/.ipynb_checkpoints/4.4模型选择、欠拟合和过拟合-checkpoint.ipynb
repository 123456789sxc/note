{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0445fb",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_37274615/article/details/81147013  \n",
    "https://www.jianshu.com/p/005d05e18c7d  \n",
    "\n",
    "训练神经网络中最最基础的三个概念：Epoch, Batch, Iteration。  \n",
    "![](.\\picture\\7.png) \n",
    "batch_size：在训练集中选择一组样本用来更新权值。1个batch包含的样本的数目，通常设为2的n次幂，常用的包括64,128,256。 网络较小时选用256，较大时选用64。\n",
    "\n",
    "iteration(迭代)：训练时，1个batch训练图像通过网络训练一次​（一次前向传播+一次后向传播），每迭代一次权重更新一次；测试时，1个batch测试图像通过网络一次​（一次前向传播）。所谓iterations就是完成一次epoch所需的batch个数  \n",
    "\n",
    "\n",
    "\n",
    "在训练模型的时候，我们要用同样一个训练集对模型进行多次训练，即进行多次epochs；为什么要进行多次epochs呢？  \n",
    "在神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。我们使用的是有限的数据集，并且我们使用一个迭代过程即梯度下降，优化学习过程和图示。因此仅仅更新权重一次或者说使用一个 epoch 是不够的。\n",
    "\n",
    "随着 epoch 数量增加，神经网络中的权重的更新次数也增加，曲线从欠拟合变得过拟合。\n",
    "\n",
    "不过我们训练的时候用的batch\n",
    "\n",
    "batch_size将影响到模型的优化程度和速度。  \n",
    "**batchsize 的正确选择是为了在内存效率和内存容量之间寻找最佳平衡。**\n",
    "合适的batchsize和随机样本能让batch能尽量代表总体样本，因为batchsize如果等于1，即我们用单独的每一个样本去确定梯度下降的方向，会导致一个样本跟总体样本的下降方向偏差很大，很难拟合。如果batchsize过大，比如为全体样本，总样本的数量小还好说，如果非常大的话，就不能一次性载入内存。\n",
    "相对于正常数据集，如果Batch_Size过小，训练数据就会非常难收敛，从而导致underfitting。\n",
    "\n",
    "增大Batch_Size，相对处理速度加快。\n",
    "\n",
    "增大Batch_Size，所需内存容量增加（epoch的次数需要增加以达到最好结果）。\n",
    "\n",
    "这里我们发现上面两个矛盾的问题，因为当epoch增加以后同样也会导致耗时增加从而速度下降。因此我们需要寻找最好的batch_size。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de564abb",
   "metadata": {},
   "source": [
    "## 神经网络的前向传播和反向传播\n",
    "神经网络可以理解为一个输入x到输出y的映射函数，即f(x)=y，其中这个映射f就是我们所要训练的网络参数w，我们只要训练出来了参数w，那么对于任何输入x，我们就能得到一个与之对应的输出y。只要f不同，那么同一个x就会产生不同的y，我们当然是想要获得最符合真实数据的y，那么我们就要训练出一个最符合真实数据的映射f。训练最符合真实数据f的过程就是神经网络的训练过程，神经网络的训练可以分为两个步骤：一个是前向传播，另外一个是反向传播  \n",
    "前向传播就是输入x，输出y的过程；\n",
    "\n",
    "反向传播就是根据损失函数来反方向地计算每一层的a、z、w、b的偏导数（梯度），从最后一层逐层向前去改变每一层的权重，也就是更新参数，其核心是损失L对每一层的每一个参数求梯度的链式求导法则。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3894a32",
   "metadata": {},
   "source": [
    "## 欠拟合、过拟合、训练误差、泛化误差\n",
    "\n",
    "\n",
    "机器学习中一个重要的话题便是模型的泛化能力，泛化能力强的模型才是好模型，对于训练好的模型，若在训练集表现差，在测试集表现同样会很差，这可能是欠拟合导致。欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。  \n",
    "\n",
    "过拟合就是现有的模型不能很好地从原始数据集推广到另一个全新的数据集。过拟合的判断标准也非常简单，即模型在训练集（原始数据集）上误差非常低，在一个全新的测试集（全新数据集）上，模型的误差又非常大。  \n",
    "**训练误差（training error）是指， 模型在训练数据集上计算得到的误差。**泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n",
    "\n",
    "问题是，我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象。 在实际中，我们只能**通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。不要拿测试集来调参数**  \n",
    "\n",
    "在我们目前已探讨、并将在之后继续探讨的监督学习情景中， 我们假设训练数据和测试数据都是从相同的分布中独立提取的。 这通常被称为独立同分布假设（i.i.d. assumption）， 这意味着对数据进行采样的过程没有进行“记忆”。 换句话说，抽取的第2个样本和第3个样本的相关性， 并不比抽取的第2个样本和第200万个样本的相关性更强。  \n",
    "要成为一名优秀的机器学习科学家需要具备批判性思考能力。 假设是存在漏洞的，即很容易找出假设失效的情况。 如果我们根据从加州大学旧金山分校医学中心的患者数据训练死亡风险预测模型， 并将其应用于马萨诸塞州综合医院的患者数据，结果会怎么样？ 这两个数据的分布可能不完全一样。 此外，抽样过程可能与时间有关。 比如当我们对微博的主题进行分类时， 新闻周期会使得正在讨论的话题产生时间依赖性，从而违反独立性假设。\n",
    "\n",
    "有时候我们即使轻微违背独立同分布假设，模型仍将继续运行得非常好。 比如，我们有许多有用的工具已经应用于现实，如人脸识别、语音识别和语言翻译。 毕竟，几乎所有现实的应用都至少涉及到一些违背独立同分布假设的情况。\n",
    "\n",
    "有些违背独立同分布假设的行为肯定会带来麻烦。 比如，我们试图只用来自大学生的人脸数据来训练一个人脸识别系统， 然后想要用它来监测疗养院中的老人。 这不太可能有效，因为大学生看起来往往与老年人有很大的不同。  \n",
    "\n",
    "当我们训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。 但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。 这种情况正是我们想要避免或控制的。 深度学习中有许多启发式的技术旨在防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c94a91",
   "metadata": {},
   "source": [
    "## 模型复杂性  \n",
    "https://blog.csdn.net/pony1001/article/details/122346782  \n",
    "模型复杂性由什么构成是一个复杂的问题。 一个模型是否能很好地泛化取决于很多因素。   \n",
    "通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂， 而需要早停（early stopping）的模型（即较少训练迭代周期）就不那么复杂。  \n",
    "这句话的意思是说，我们有一个神经网络模型，如果我们对神经网络进行训练，随着训练时间的增长，训练误差会越来越小，验证误差会先减少后增大。如果训练的模型位于验证误差最低点的右边，就说这个模型是过拟合的。  \n",
    "早停法：  \n",
    "其基本含义是在训练中计算模型在验证集上的表现，当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。  \n",
    "其主要步骤如下：\n",
    "\n",
    "1. 将原始的训练数据集划分成训练集和验证集\n",
    "2. 只在训练集上进行训练，并每个一个周期计算模型在验证集上的误差，例如，每15次epoch（mini batch训练中的一个周期）\n",
    "3. 当模型在验证集上的误差比上一次训练结果差的时候停止训练\n",
    "4. 使用上一次迭代结果中的参数作为模型的最终参数\n",
    "\n",
    "在现实中，模型在验证集上的误差不会像上图那样平滑，而是像下图一样\n",
    "![](./picture/6.png)\n",
    "也就是说，模型在验证集上的表现可能咱短暂的变差之后有可能继续变好。上图在训练集迭代到400次的时候出现了16个局部最低。其中有4个最低值是它们所在位置出现的时候的最低点。其中全局最优大约出现在第205次迭代中。首次出现最低点是第45次迭代。相比较第45次迭代停止，到第400次迭代停止的时候找出的最低误差比第45次提高了1.1%，但是训练时间大约是前者的7倍。\n",
    "我们该选择哪一个点作为我们的最优模型呢？换句话说，我们早停的标准是什么呢？**早停法主要是训练时间和泛化错误之间的权衡。**\n",
    "我们需要一个停止的标准来实施早停法，因此，我们希望它可以产生最低的泛化错误，同时也可以有最好的性价比，即给定泛化错误下的最小训练时间。  \n",
    "了解了早停法之后，我们再来看对神经网络复杂性的判定。  \n",
    "通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂， 而需要早停（early stopping）的模型（即较少训练迭代周期）就不那么复杂。  \n",
    "对于神经网络来说，如果对一个模型进行训练至到达了我们预期的模型效果，这一过程训练的iteration的次数越大，我们就说模型越复杂。\n",
    "\n",
    "一个模型容易泛化，这个模型就是简单模型，反之就是复杂模型。\n",
    "\n",
    "上面是对神经网络的复杂性的介绍，下面是对不同类型的模型复杂度的介绍。\n",
    "我们很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。 就目前而言，一条简单的经验法则相当有用： 统计学家认为，能够轻松解释任意事实的模型是复杂的， 而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。   \n",
    "举个例子，线性模型和神经网络相比，神经网络更加复杂，因为线性模型只能用来预测和分类，而神经网络不只能干线性模型的工作还能用于非线性的分类等。线性模型的表达能力有限，但是能很好的对数据进行分类或预测，现实中我们一般会用线性模型进行预测和分类，用复杂的模型没必要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b7893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "d2l-zh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
