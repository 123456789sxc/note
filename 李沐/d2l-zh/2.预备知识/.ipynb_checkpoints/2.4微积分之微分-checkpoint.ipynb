{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4884508f",
   "metadata": {},
   "source": [
    "在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个损失函数（loss function）， 即一个衡量“模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：\n",
    "\n",
    "优化（optimization）：用模型拟合观测数据的过程；\n",
    "\n",
    "泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce205de",
   "metadata": {},
   "source": [
    "## 导数和微分\n",
    "我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。 在深度学习中，我们通常选择对于模型参数可微的损失函数。  \n",
    "我们希望调整参数来尽可能地让损失函数小，这样得到的模型对训练数据会产生更准确的输出。因此为了得到这样的模型，我们需要找到损失函数最小时的参数\n",
    "**简而言之，对于每个参数， 如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。(不理解)**  \n",
    "假设我们有一个函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$，其输入和输出都是标量。  \n",
    "(**如果$f$的*导数*存在，这个极限被定义为**)\n",
    "\n",
    "(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**)\n",
    "\n",
    "如果$f'(a)$存在，则称$f$在$a$处是*可微*（differentiable）的。\n",
    "如果$f$在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。\n",
    "我们可以将导数$f'(x)$解释为$f(x)$相对于$x$的*瞬时*（instantaneous）变化率。  \n",
    "所谓的瞬时变化率是指在$x$的基础上，增加或减少一个h，h是非常小的(趋于0的)，$f(x)$的变化量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa52948",
   "metadata": {},
   "source": [
    "让我们熟悉一下导数的几个等价符号。\n",
    "给定$y=f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量。以下表达式是等价的：\n",
    "\n",
    "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
    "\n",
    "其中符号$\\frac{d}{dx}$和$D$是*微分运算符*，表示*微分*操作。\n",
    "我们可以使用以下规则来对常见函数求微分：\n",
    "\n",
    "* $DC = 0$（$C$是一个常数）\n",
    "* $Dx^n = nx^{n-1}$（*幂律*（power rule），$n$是任意实数）\n",
    "* $De^x = e^x$\n",
    "* $D\\ln(x) = 1/x$\n",
    "\n",
    "为了微分一个由一些常见函数组成的函数，下面的一些法则方便使用。\n",
    "假设函数$f$和$g$都是可微的，$C$是一个常数，则：\n",
    "\n",
    "*常数相乘法则*\n",
    "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
    "\n",
    "*加法法则*\n",
    "\n",
    "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
    "\n",
    "*乘法法则*\n",
    "\n",
    "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
    "\n",
    "*除法法则*\n",
    "\n",
    "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
    "\n",
    "\n",
    "[**为了对导数的这种解释进行可视化，我们将使用`matplotlib`**]，\n",
    "这是一个Python中流行的绘图库。\n",
    "要配置`matplotlib`生成图形的属性，我们需要(**定义几个函数**)。\n",
    "在下面，`use_svg_display`函数指定`matplotlib`软件包输出svg图表以获得更清晰的图像。\n",
    "\n",
    "注意，注释`#@save`是一个特殊的标记，会将对应的函数、类或语句保存在`d2l`包中。\n",
    "因此，以后无须重新定义就可以直接调用它们（例如，`d2l.use_svg_display()`）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7a376",
   "metadata": {},
   "source": [
    "## 偏导数\n",
    "\n",
    "到目前为止，我们只讨论了仅含一个变量的函数的微分。\n",
    "在深度学习中，函数通常依赖于许多变量。\n",
    "因此，我们需要将微分的思想推广到*多元函数*（multivariate function）上。\n",
    "\n",
    "设$y = f(x_1, x_2, \\ldots, x_n)$是一个具有$n$个变量的函数。\n",
    "$y$关于第$i$个参数$x_i$的*偏导数*（partial derivative）为：\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
    "\n",
    "为了计算$\\frac{\\partial y}{\\partial x_i}$，\n",
    "**我们可以简单地将$x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$看作常数，\n",
    "并计算$y$关于$x_i$的导数。计算得到的偏导数是一个标量**  \n",
    "对于偏导数的表示，以下是等价的：\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
    "\n",
    "## 梯度\n",
    "个人理解:标量y是向量x的函数，y对向量x的导数，叫梯度；下面是具体定义\n",
    "\n",
    "**我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。**  \n",
    "具体而言，设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n",
    "一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$，并且输出的偏导数是一个标量。\n",
    "函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
    "\n",
    "其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n",
    "\n",
    "假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n",
    "\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$ ---->A是mXn的矩阵，Ax是矩阵叉乘列向量，即f(x) = Ax; $\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$ 下面同理\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n",
    "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n",
    "\n",
    "同样，对于任何矩阵$\\mathbf{X}$，都有$\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。\n",
    "正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。\n",
    "\n",
    "## 链式法则\n",
    "\n",
    "然而，上面方法可能很难找到梯度。\n",
    "这是因为在深度学习中，多元函数通常是*复合*（composite）的，\n",
    "所以难以应用上述任何规则来微分这些函数。\n",
    "幸运的是，链式法则可以被用来微分复合函数。\n",
    "\n",
    "让我们先考虑单变量函数。假设函数$y=f(u)$和$u=g(x)$都是可微的，根据链式法则：\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
    "\n",
    "现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。\n",
    "假设可微分函数$y$有变量$u_1, u_2, \\ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \\ldots, x_n$。\n",
    "注意，$y$是$x_1, x_2， \\ldots, x_n$的函数。\n",
    "对于任意$i = 1, 2, \\ldots, n$，链式法则给出：\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}$$\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。\n",
    "* 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。\n",
    "* **梯度是一个向量，其分量是多变量函数相对于其每一个变量的偏导数。**\n",
    "* 链式法则可以用来微分复合函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69d764",
   "metadata": {},
   "source": [
    "## 函数可视化的学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7faebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "\n",
    "\n",
    "## 挖坑了,学习matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a697a7d",
   "metadata": {},
   "source": [
    "![jupyter](./picture/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1595a06",
   "metadata": {},
   "source": [
    "**我们根据两个例子来对上面所学内容进行理解，以及知晓求导的过程**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e3173",
   "metadata": {},
   "source": [
    "![jupyter](./picture/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abc4cb",
   "metadata": {},
   "source": [
    "解析：  \n",
    "x和w是n维向量，y是标量，z等于 (x和w的内积-y)的平方，要求的是z对w的导数；  \n",
    "由上可知，z是一个标量，要求的是标量z对w的导数，根据上图可知，求的是标量z对w的每一维求偏导，结果是一个跟w一样向量，即求的是梯度；  \n",
    "我们可以把式子进行分解，将其看成复合函数，用链式法则进行求导，得到结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e837ad",
   "metadata": {},
   "source": [
    "![jupyter](./picture/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543d1af",
   "metadata": {},
   "source": [
    "这个就不解析了，**理解求导的过程，有助于下一章自动求导的理解**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d408d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "d2l-zh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
